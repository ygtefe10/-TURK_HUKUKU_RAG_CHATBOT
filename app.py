# ==============================================================================
#      *** TÃœRK HUKUKU RAG CHATBOT - PROJE KODU (v8) - TEK HÃœCRE ***
# ==============================================================================
# Proje: TÃ¼rk Hukuk metinleri Ã¼zerinde RAG (Retrieval-Augmented Generation)
#        yÃ¶ntemiyle Ã§alÄ±ÅŸan bir chatbot.
# SÃ¼rÃ¼m v8: Paket kurulumlarÄ± (pip) en baÅŸa alÄ±ndÄ±,
#            vektÃ¶r veritabanÄ± olarak yerel (local) ChromaDB kullanÄ±lÄ±yor.
# Ortam: Google Colab
# ==============================================================================

import time  # Kodun Ã§alÄ±ÅŸma sÃ¼resini ve zaman damgalarÄ±nÄ± yÃ¶netmek iÃ§in
import os    # Ä°ÅŸletim sistemiyle ilgili iÅŸlemler (API anahtarÄ±, dosya yollarÄ±)
import shutil # Dosya/klasÃ¶r silme iÅŸlemleri (Ã¶rn: eski DB'yi temizleme)
import subprocess # Sistem komutlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in
import threading  # (Åu an kullanÄ±lmÄ±yor ancak Streamlit gibi arayÃ¼zler iÃ§in gerekebilir)
import pandas as pd # Veri iÅŸleme iÃ§in (ÅŸu an doÄŸrudan kullanÄ±lmÄ±yor ancak gelecekte veri analizi iÃ§in tutuluyor)

print(f"--- Proje BaÅŸlangÄ±Ã§ ZamanÄ±: {time.strftime('%Y-%m-%d %H:%M:%S')} ---")

# ==============================================================================
# AdÄ±m 1: Gerekli BaÄŸÄ±mlÄ±lÄ±klarÄ±n YÃ¼klenmesi
# ==============================================================================
# Colab ortamÄ± her baÅŸladÄ±ÄŸÄ±nda paketlerin yeniden kurulmasÄ± gerekir.
# Bu adÄ±mÄ± en baÅŸa alarak, kodun geri kalanÄ±nÄ±n ihtiyaÃ§ duyduÄŸu
# kÃ¼tÃ¼phanelerin (LangChain, ChromaDB, Gemini vb.) yÃ¼klÃ¼ olduÄŸundan emin oluyoruz.
print("\n--- AdÄ±m 1: Gerekli Paketler YÃ¼kleniyor ---")
print("Gerekli tÃ¼m kÃ¼tÃ¼phaneler yÃ¼kleniyor (Bu iÅŸlem biraz sÃ¼rebilir)...")


print("âœ… Gerekli ana paketler yÃ¼klendi/gÃ¼ncellendi.")

# Paketler yÃ¼klendikten sonra, onlarÄ± kod iÃ§inde kullanabilmek iÃ§in import ediyoruz.
# Bu importlarÄ± bir try-except bloÄŸuna alÄ±yoruz ki,
# eÄŸer bir kÃ¼tÃ¼phane eksikse veya yÃ¼klenememiÅŸse programÄ± en baÅŸta durdurabilelim.
try:
    from pyngrok import ngrok, conf # (Streamlit'i dÄ±ÅŸarÄ±ya aÃ§mak iÃ§in gerekli)
    from datasets import load_dataset # Hugging Face'den veri setini Ã§ekmek iÃ§in
    import google.generativeai as genai # Gemini LLM'i kullanmak iÃ§in
    import chromadb # Yerel vektÃ¶r veritabanÄ± iÃ§in
    from langchain_text_splitters import RecursiveCharacterTextSplitter # Metinleri parÃ§alara (chunk) ayÄ±rmak iÃ§in
    from langchain_core.documents import Document # LangChain'in standart belge formatÄ±
    from langchain_core.prompts import PromptTemplate # LLM'e gÃ¶ndereceÄŸimiz ÅŸablon
    from langchain_core.runnables import RunnablePassthrough, RunnableLambda # LangChain Expression Language (LCEL) zincirini kurmak iÃ§in

    # Colab'e Ã¶zel 'userdata' secret yÃ¶neticisini kontrol et.
    # EÄŸer Colab'de deÄŸilsek (Ã¶rn: local VSCode), bu import hata verecektir.
    try:
        from google.colab import userdata; USE_SECRETS = True
        print("Google Colab 'userdata' (Secrets) modÃ¼lÃ¼ bulundu.")
    except ImportError:
        USE_SECRETS = False # Colab'de deÄŸiliz, API anahtarÄ±nÄ± manuel isteyeceÄŸiz.
        print("Google Colab 'userdata' bulunamadÄ±. API anahtarÄ± manuel istenecek.")

    print("âœ… Gerekli kÃ¼tÃ¼phaneler baÅŸarÄ±yla import edildi.")
except ImportError as e:
    print(f"âŒ KÃ¼tÃ¼phane import hatasÄ±: {e}. LÃ¼tfen paket kurulumunu kontrol edin.")
    # Kritik bir kÃ¼tÃ¼phane eksikse, programÄ±n devam etmesi anlamsÄ±z.
    raise SystemExit("Kritik kÃ¼tÃ¼phaneler yÃ¼klenemedi. Notebook durduruluyor.")
print("-" * 50)


# ==============================================================================
# AdÄ±m 2: Gemini API AnahtarÄ±nÄ±n AyarlanmasÄ±
# ==============================================================================
# GÃ¼venlik nedeniyle API anahtarÄ±nÄ± koda doÄŸrudan yazmÄ±yoruz.
# Her Ã§alÄ±ÅŸtÄ±rmada kullanÄ±cÄ±dan manuel olarak girmesini istiyoruz (input).
# Bu, kodun paylaÅŸÄ±mÄ±nÄ± kolaylaÅŸtÄ±rÄ±r ve anahtarÄ±n sÄ±zmasÄ±nÄ± engeller.
print("\n--- AdÄ±m 2: Gemini API AnahtarÄ± AyarlanÄ±yor ---")
API_KEY_LOADED = False
try:
    # KullanÄ±cÄ±dan API anahtarÄ±nÄ± gÃ¼venli bir ÅŸekilde al
    api_key_input = input("LÃ¼tfen Gemini API AnahtarÄ±nÄ±zÄ± Girin: ")
    if not api_key_input:
        raise ValueError("API AnahtarÄ± girilmedi.")

    # AlÄ±nan anahtarÄ± iÅŸletim sistemi ortam deÄŸiÅŸkeni (environment variable) olarak ayarlÄ±yoruz.
    # Bu, 'genai' kÃ¼tÃ¼phanesinin anahtarÄ± otomatik olarak bulmasÄ± iÃ§in standart bir yoldur.
    os.environ["GEMINI_API_KEY"] = api_key_input
    api_key_env = os.getenv('GEMINI_API_KEY')

    if not api_key_env:
        print("âŒ Hata: API anahtarÄ± ortam deÄŸiÅŸkeni olarak ayarlanamadÄ±.")
    else:
        # genai kÃ¼tÃ¼phanesini bu anahtarla yapÄ±landÄ±r
        genai.configure(api_key=api_key_env)
        print("âœ… Gemini API baÅŸarÄ±yla yapÄ±landÄ±rÄ±ldÄ±.")
        API_KEY_LOADED = True # Sonraki adÄ±mlar iÃ§in bayraÄŸÄ± ayarla
except Exception as e:
    print(f"âŒ Gemini API yapÄ±landÄ±rma hatasÄ±: {e}")
print("-" * 50)

# ==============================================================================
# Ek AdÄ±m: Google Cloud Kimlik DoÄŸrulamasÄ± (ADC)
# ==============================================================================
# Gemini API anahtarÄ± (AdÄ±m 2) LLM (gemini-2.0-flash) iÃ§in kullanÄ±lÄ±r.
# Ancak, yeni nesil embedding modelleri (Ã¶rn: text-embedding-004)
# genellikle ek olarak Google Cloud projesi Ã¼zerinden kimlik doÄŸrulamasÄ±
# (Application Default Credentials - ADC) gerektirir. Bu adÄ±m bu ek doÄŸrulamayÄ± yapar.
GCLOUD_AUTH_DONE = False
if API_KEY_LOADED: # Sadece API anahtarÄ± baÅŸarÄ±yla yÃ¼klendiyse devam et
    print("\n--- Ek AdÄ±m: Google Cloud Kimlik DoÄŸrulamasÄ± (ADC) ---")
    try:
        # ADC kimlik bilgilerinin Colab'de saklandÄ±ÄŸÄ± varsayÄ±lan yol
        adc_path = "/content/.config/application_default_credentials.json"

        # EÄŸer bu dosya zaten varsa, daha Ã¶nce doÄŸrulama yapÄ±lmÄ±ÅŸ demektir.
        # Tekrar sormamak iÃ§in bu adÄ±mÄ± atlÄ±yoruz.
        if os.path.exists(adc_path):
            print("Mevcut ADC dosyasÄ± bulundu, kimlik doÄŸrulamasÄ± atlanÄ±yor.")
            GCLOUD_AUTH_DONE = True
        else:
            # Dosya yoksa, kullanÄ±cÄ±dan kimlik doÄŸrulamasÄ± istiyoruz.
            # 'gcloud auth' komutu bir link aÃ§ar, kullanÄ±cÄ± izin verir ve bir kod yapÄ±ÅŸtÄ±rÄ±r.
             print("LÃ¼tfen Ã§Ä±kan linke tÄ±klayÄ±p Google hesabÄ±nÄ±zla izin verin ve doÄŸrulama kodunu buraya yapÄ±ÅŸtÄ±rÄ±n.")
             get_ipython().system('gcloud auth application-default login --quiet --no-launch-browser')

             if os.path.exists(adc_path):
                 print("\nâœ… ADC kimlik doÄŸrulamasÄ± baÅŸarÄ±yla tamamlandÄ±.")
                 GCLOUD_AUTH_DONE = True
             else:
                 print("\nâš ï¸ ADC dosyasÄ± oluÅŸturulamadÄ±. Embedding adÄ±mÄ±nda sorun yaÅŸanabilir.")
    except Exception as gcloud_e:
        print(f"âš ï¸ gcloud kimlik doÄŸrulama hatasÄ±: {gcloud_e}")
    print("-" * 50)
else:
    print("API AnahtarÄ± yÃ¼klenemediÄŸi iÃ§in Google Cloud ADC adÄ±mÄ± atlandÄ±.")
    print("-" * 50)

# ==============================================================================
# AdÄ±m 3: Veri Setinin YÃ¼klenmesi
# ==============================================================================
# Bu projede, Hugging Face Hub Ã¼zerinde bulunan Ã¶zel bir TÃ¼rk hukuku
# Soru-Cevap veri setini kullanÄ±yoruz.
print("\n--- AdÄ±m 3: Veri Seti YÃ¼kleniyor ---")
dataset_name = "Renicames/turkish-law-chatbot" # Veri setinin Hugging Face ID'si
data = None
DATA_LOADED = False
print(f"Hugging Face Hub'dan '{dataset_name}' veri seti yÃ¼kleniyor...")
try:
    # 'load_dataset' fonksiyonu ile veriyi Ã§ek
    dataset = load_dataset(dataset_name)
    print("âœ… Veri seti yÃ¼klendi.")

    # Genellikle veri 'train' bÃ¶lÃ¼nmesinde (split) bulunur
    if 'train' in dataset:
        data = dataset['train']
        print(f"Veri setinin 'train' bÃ¶lÃ¼mÃ¼ alÄ±ndÄ±. Toplam kayÄ±t: {len(data)}")

        # Verinin RAG iÃ§in uygun olup olmadÄ±ÄŸÄ±nÄ± kontrol et.
        # Bizim 'Cevap' sÃ¼tunundaki metinlere (asÄ±l hukuk metni) ihtiyacÄ±mÄ±z var.
        if 'Soru' in data.column_names and 'Cevap' in data.column_names:
            print("âœ… Gerekli 'Soru' ve 'Cevap' sÃ¼tunlarÄ± bulundu."); DATA_LOADED = True
        else:
            print("âŒ Gerekli 'Soru'/'Cevap' sÃ¼tunlarÄ± bulunamadÄ±."); data = None
    else:
        print("âŒ Veri setinde 'train' bÃ¶lÃ¼mÃ¼ bulunamadÄ±."); data = None
except Exception as e:
    print(f"âŒ Veri seti yÃ¼klenirken hata oluÅŸtu: {e}"); data = None

if not DATA_LOADED:
    print("âŒ Veri seti yÃ¼klenemedi. Sonraki adÄ±mlar Ã§alÄ±ÅŸmayabilir.")
print("-" * 50)

# ==============================================================================
# AdÄ±m 4: Verinin HazÄ±rlanmasÄ± ve ParÃ§alara AyrÄ±lmasÄ± (Chunking)
# ==============================================================================
# RAG modelinin Ã§alÄ±ÅŸmasÄ± iÃ§in, uzun hukuk metinlerini (Cevaplar)
# daha kÃ¼Ã§Ã¼k, yÃ¶netilebilir parÃ§alara (chunks) bÃ¶lmemiz gerekiyor.
# Bu, embedding modelinin (AdÄ±m 5) metinleri daha iyi anlamlandÄ±rmasÄ±nÄ±
# ve vektÃ¶r veritabanÄ±nÄ±n daha verimli Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.
print("\n--- AdÄ±m 4: Veri HazÄ±rlanÄ±yor ve ParÃ§alanÄ±yor (TÃ¼m Veri) ---")
SORU_COLUMN = 'Soru'; CEVAP_COLUMN = 'Cevap'
documents_for_rag = []; doc_chunks = []
CHUNKS_CREATED = False

if DATA_LOADED and data is not None:
    # 1. Veriyi LangChain 'Document' formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme
    # 'page_content' = AsÄ±l metin (vektÃ¶r veritabanÄ±na girecek olan)
    # 'metadata' = Metinle iliÅŸkili ek bilgi (kaynak soru, ID, vb.)
    print(f"'{CEVAP_COLUMN}' sÃ¼tunundaki metinler 'Document' formatÄ±na getiriliyor...")
    for i, item in enumerate(data):
        page_content = item[CEVAP_COLUMN] # AsÄ±l hukuk metni
        # Orijinal soruyu metadata'ya ekliyoruz. Bu, kaynak takibi iÃ§in yararlÄ±.
        metadata = {"source_id": i, SORU_COLUMN: item[SORU_COLUMN]}
        documents_for_rag.append(Document(page_content=page_content, metadata=metadata))
    print(f"âœ… {len(documents_for_rag)} adet LangChain 'Document' nesnesi oluÅŸturuldu.")

    # 2. Metin ParÃ§alayÄ±cÄ±yÄ± (Text Splitter) TanÄ±mlama
    # RecursiveCharacterTextSplitter: Metinleri "\n\n", sonra "\n", sonra " "
    # gibi ayraÃ§lara gÃ¶re bÃ¶lmeye Ã§alÄ±ÅŸÄ±r. Anlamsal bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ korumaya Ã§alÄ±ÅŸÄ±r.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, # Her bir parÃ§anÄ±n maksimum karakter sayÄ±sÄ±
        chunk_overlap=150  # ParÃ§alar arasÄ± ortak karakter sayÄ±sÄ± (anlam kaybÄ±nÄ± Ã¶nlemek iÃ§in)
    )

    # 3. ParÃ§alama Ä°ÅŸlemi
    try:
        doc_chunks = text_splitter.split_documents(documents_for_rag)
        print(f"âœ… {len(documents_for_rag)} belge, {len(doc_chunks)} adet metin parÃ§asÄ±na (chunk) ayrÄ±ldÄ±.")
        if doc_chunks:
            # Ä°lk parÃ§anÄ±n metaverisini basarak iÅŸlemin doÄŸruluÄŸunu kontrol et
            print("Test: Ä°lk ParÃ§anÄ±n Metaverisi:", doc_chunks[0].metadata); CHUNKS_CREATED = True
        else:
            print("âŒ ParÃ§alama iÅŸlemi boÅŸ bir liste dÃ¶ndÃ¼rdÃ¼.")
    except Exception as e:
        print(f"âŒ Metin bÃ¶lme iÅŸlemi sÄ±rasÄ±nda hata: {e}"); doc_chunks = []
else:
    print("âŒ Veri seti yÃ¼klenemediÄŸi iÃ§in (AdÄ±m 3) parÃ§alama yapÄ±lamÄ±yor.")

if not CHUNKS_CREATED:
    print("âŒ Metin parÃ§alarÄ± (chunks) oluÅŸturulamadÄ±.")
print("-" * 50)

# ==============================================================================
# AdÄ±m 5: Embedding OluÅŸturma ve Yerel VektÃ¶r VeritabanÄ± Kurulumu
# ==============================================================================
# Bu adÄ±mda, AdÄ±m 4'te oluÅŸturduÄŸumuz her bir metin parÃ§asÄ±nÄ± (chunk)
# Gemini'nin 'text-embedding-004' modelini kullanarak sayÄ±sal bir vektÃ¶re
# (embedding) dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz.
# ArdÄ±ndan bu vektÃ¶rleri, yerel bir ChromaDB veritabanÄ±na kaydedeceÄŸiz.
print("\n--- AdÄ±m 5: Embedding ve YEREL VektÃ¶r VeritabanÄ± Kurulumu ---")

# --- YardÄ±mcÄ± Fonksiyon: Embedding (Hata YÃ¶netimli) ---
# Google API'leri sÄ±k kullanÄ±mda "429 - Resource Exhausted" (Rate Limit) hatasÄ± verebilir.
# Bu fonksiyon, bu hatayÄ± yakalayÄ±p, bir sÃ¼re bekleyip (exponential backoff)
# iÅŸlemi tekrar denemek (retry) iÃ§in yazÄ±lmÄ±ÅŸtÄ±r. Projenin saÄŸlamlÄ±ÄŸÄ± iÃ§in kritiktir.
def embed_content_with_retry(content, model="models/text-embedding-004", task_type="RETRIEVAL_DOCUMENT", max_retries=5, initial_delay=2):
    delay = initial_delay; last_exception = None; is_batch = isinstance(content, list)
    for attempt in range(max_retries):
        try:
            # Sorgu (QUERY) ve Belge (DOCUMENT) embedding'leri farklÄ± 'task_type' gerektirir.
            # EÄŸer tek bir metin gelirse ve tipi 'QUERY' ise onu kullan,
            # aksi halde (liste veya tek metin fark etmeksizin) 'DOCUMENT' olarak etiketle.
            current_task_type = task_type if isinstance(content, str) and task_type == 'RETRIEVAL_QUERY' else 'RETRIEVAL_DOCUMENT'

            result = genai.embed_content(model=model, content=content, task_type=current_task_type)

            # DÃ¶nen sonucun formatÄ± tekil (embedding) veya Ã§oÄŸul (embeddings) olabilir
            embedding_key = 'embedding' if 'embedding' in result else ('embeddings' if 'embeddings' in result else None)
            if embedding_key:
                return result[embedding_key] # BaÅŸarÄ±lÄ± embedding'i dÃ¶ndÃ¼r
            else:
                raise ValueError("Embedding sonucu beklenen 'embedding' veya 'embeddings' anahtarÄ±nÄ± iÃ§ermiyor.")

        except Exception as e:
            error_str = str(e); print(f"Embedding hatasÄ± (Deneme {attempt + 1}/{max_retries}): {error_str[:200]}...")
            # Rate limit hatasÄ± (429) veya kaynak tÃ¼kenmesi hatasÄ±nÄ± yakala
            if "Resource has been exhausted" in error_str or "429" in error_str:
                wait_time = delay * 5 # Rate limit iÃ§in daha uzun bekle
                print(f"Rate limit tespit edildi, {wait_time} saniye bekleniyor...");
                time.sleep(wait_time); delay *= 2 # Bekleme sÃ¼resini Ã¼ssel olarak artÄ±r
            elif attempt < max_retries - 1: # DiÄŸer geÃ§ici hatalar iÃ§in
                print(f"GeÃ§ici hata, {delay} saniye bekleniyor...");
                time.sleep(delay); delay *= 2 # Bekleme sÃ¼resini Ã¼ssel olarak artÄ±r
            else:
                print("Maksimum deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±."); last_exception = e; break # DÃ¶ngÃ¼den Ã§Ä±k

    if last_exception: # TÃ¼m denemelere raÄŸmen baÅŸarÄ±sÄ±z olduysa
        if is_batch: return [None] * len(content) # Batch ise 'None' listesi dÃ¶ndÃ¼r
        else: raise last_exception # Tekil ise hatayÄ± fÄ±rlat
    return None
# --- YardÄ±mcÄ± Fonksiyon BitiÅŸi ---

chroma_client = None
chroma_collection = None
# VeritabanÄ±nÄ± Colab ortamÄ±nda yerel bir klasÃ¶rde sakla
db_path = "./chroma_db_law_local_full"
collection_name = "hukuk_tr_collection_full_local" # DB iÃ§indeki koleksiyon adÄ±

# Mevcut sÃ¼rÃ¼mde, her Ã§alÄ±ÅŸtÄ±rmada veritabanÄ±nÄ± sÄ±fÄ±rdan oluÅŸturuyoruz.
# Bu, veride veya chunking ayarlarÄ±nda deÄŸiÅŸiklik yapÄ±ldÄ±ÄŸÄ±nda tutarlÄ±lÄ±k saÄŸlar.
DB_CREATED_OR_LOADED = False

if CHUNKS_CREATED and doc_chunks and (API_KEY_LOADED or GCLOUD_AUTH_DONE):
    print(f"Yerel Chroma veritabanÄ± yolu: {db_path}")
    try:
        # 1. Eski VeritabanÄ±nÄ± Temizle
        # EÄŸer bu klasÃ¶r varsa, iÃ§indekileri sil (shutil.rmtree)
        if os.path.exists(db_path):
            print(f"Eski veritabanÄ± '{db_path}' siliniyor...");
            shutil.rmtree(db_path)
        os.makedirs(db_path, exist_ok=True) # KlasÃ¶rÃ¼ (yeniden) oluÅŸtur

        print("\n--- Embedding Ä°ÅŸlemi BaÅŸlatÄ±lÄ±yor (TÃœM VERÄ° - UZUN SÃœREBÄ°LÄ°R!) ---")
        # Chroma'ya gÃ¶ndermek iÃ§in metinleri, metalarÄ± ve ID'leri ayÄ±r
        chunk_texts = [doc.page_content for doc in doc_chunks]
        chunk_metadatas = [doc.metadata for doc in doc_chunks]
        chunk_ids = [f"doc_{i}" for i in range(len(doc_chunks))] # Her chunk iÃ§in benzersiz ID
        print(f"{len(chunk_texts)} adet metin parÃ§asÄ± (chunk) bulundu.")

        # 2. Embedding'leri Batch (Grup) Halinde OluÅŸtur
        print("\nMetin parÃ§alarÄ± iÃ§in vektÃ¶rler (embeddings) oluÅŸturuluyor...")
        start_time = time.time()
        all_embeddings = []
        batch_size_embed = 100 # API'ye tek seferde 100 metin gÃ¶nder (Rate limit'i aÅŸmamak iÃ§in)
        num_batches_embed = (len(chunk_texts) + batch_size_embed - 1) // batch_size_embed

        for i in range(0, len(chunk_texts), batch_size_embed):
            batch_index_embed = i // batch_size_embed + 1
            batch_texts = chunk_texts[i:i+batch_size_embed]
            try:
                # Hata yÃ¶netimli fonksiyonumuzu kullanarak embedding'leri al
                batch_embeddings = embed_content_with_retry(batch_texts, task_type="RETRIEVAL_DOCUMENT")
                all_embeddings.extend(batch_embeddings)
                print(f"Embedding Batch {batch_index_embed}/{num_batches_embed} iÅŸlendi (Toplam {len(all_embeddings)}/{len(chunk_texts)}).")
            except Exception as e:
                print(f"âŒ Batch {batch_index_embed} iÅŸlenirken kritik hata: {e}.")
                all_embeddings.extend([None] * len(batch_texts)) # BaÅŸarÄ±sÄ±z olanlarÄ± 'None' ile doldur

            time.sleep(1) # API'ye saygÄ± iÃ§in her batch arasÄ± 1 saniye bekle

        end_time = time.time()
        print(f"Embedding iÅŸlemi toplam {end_time - start_time:.2f} saniye sÃ¼rdÃ¼.")

        # 3. BaÅŸarÄ±sÄ±z Embedding'leri Filtrele
        # 'None' olarak iÅŸaretlenen (baÅŸarÄ±sÄ±z olan) embedding'leri ayÄ±kla
        valid_indices = [i for i, emb in enumerate(all_embeddings) if emb is not None]
        valid_texts = [chunk_texts[i] for i in valid_indices]
        valid_metadatas = [chunk_metadatas[i] for i in valid_indices]
        valid_embeddings = [all_embeddings[i] for i in valid_indices]
        valid_ids = [chunk_ids[i] for i in valid_indices]
        if len(valid_indices) != len(chunk_texts):
            print(f"âš ï¸ UyarÄ±: {len(chunk_texts) - len(valid_indices)} adet metin iÃ§in embedding hesaplanamadÄ±.")

        # 4. ChromaDB'ye Verileri Ekle
        if valid_ids:
             print(f"{len(valid_ids)} adet geÃ§erli embedding bulundu.")
             print(f"\nYerel Chroma veritabanÄ± '{db_path}' iÃ§ine oluÅŸturuluyor...")
             try:
                 # 'PersistentClient', verileri diske (belirtilen yola) kaydeder.
                 chroma_client = chromadb.PersistentClient(path=db_path)
                 print(f"'{collection_name}' koleksiyonu oluÅŸturuluyor/yÃ¼kleniyor...")
                 # 'get_or_create_collection' ile koleksiyonu gÃ¼venle al (varsa yÃ¼kler, yoksa oluÅŸturur)
                 chroma_collection = chroma_client.get_or_create_collection(name=collection_name)
                 # chroma_collection = chroma_client.create_collection(name=collection_name) # Bu hata verir eÄŸer varsa

                 # VeritabanÄ±na ekleme iÅŸlemini de batch halinde yapÄ±yoruz.
                 # Binlerce veriyi tek seferde eklemek Chroma'yÄ± yorabilir.
                 batch_size_chroma = 4000
                 num_chroma_batches = (len(valid_ids) + batch_size_chroma - 1) // batch_size_chroma
                 print(f"{len(valid_ids)} Ã¶ÄŸe Chroma'ya {num_chroma_batches} batch halinde eklenecek...")
                 total_added = 0

                 for i in range(0, len(valid_ids), batch_size_chroma):
                      batch_index_chroma = i // batch_size_chroma + 1
                      # Ä°lgili batch iÃ§in ID, embedding, metin ve metaveriyi seÃ§
                      ids_batch=valid_ids[i:i+batch_size_chroma]
                      embeddings_batch=valid_embeddings[i:i+batch_size_chroma]
                      documents_batch=valid_texts[i:i+batch_size_chroma]
                      metadatas_batch=valid_metadatas[i:i+batch_size_chroma]

                      print(f"Chroma Batch {batch_index_chroma}/{num_chroma_batches} ({len(ids_batch)} Ã¶ÄŸe) ekleniyor...")
                      try:
                          # Verileri koleksiyona ekle
                          chroma_collection.add(ids=ids_batch, embeddings=embeddings_batch, documents=documents_batch, metadatas=metadatas_batch)
                          total_added += len(ids_batch)
                          print(f"Batch {batch_index_chroma} eklendi. Toplam eklenen: {total_added}/{len(valid_ids)}")
                      except Exception as add_e:
                          print(f"âŒ Hata (Chroma Batch {batch_index_chroma} eklenirken): {add_e}")

                 if total_added == len(valid_ids):
                     print(f"âœ… TÃ¼m veriler ({total_added}) yerel Chroma veritabanÄ±na baÅŸarÄ±yla eklendi.")
                 else:
                     print(f"âš ï¸ UyarÄ±: Veri kaybÄ± var. Eklenen: {total_added}/{len(valid_ids)}")

                 # 5. VeritabanÄ± Test Sorgusu (Sanity Check)
                 # DB'nin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in basit bir arama yap
                 print("\nYerel DB testi: 'anayasa mahkemesi' kelimesi aranÄ±yor...")
                 query_text = "anayasa mahkemesi"
                 try:
                      # Sorguyu da (farklÄ± task_type ile) embed et
                      query_embedding = embed_content_with_retry(query_text, task_type='RETRIEVAL_QUERY')
                      if query_embedding:
                           # 'query' metodu ile en yakÄ±n 2 sonucu (n_results=2) getir
                           results = chroma_collection.query(query_embeddings=[query_embedding], n_results=2)
                           if results and results.get('ids') and results['ids'][0]:
                               print(f"âœ… VeritabanÄ± testi baÅŸarÄ±lÄ±. {len(results['ids'][0])} sonuÃ§ bulundu.");
                               DB_CREATED_OR_LOADED = True # Her ÅŸey yolunda, RAG'a geÃ§ebiliriz.
                           else:
                               print("âš ï¸ Test sorgusu sonuÃ§ dÃ¶ndÃ¼rmedi (DB boÅŸ olabilir).")
                      else:
                          print("âŒ Test sorgusu embed edilemedi.")
                 except Exception as search_e:
                     print(f"âŒ VeritabanÄ± test sorgusu hatasÄ±: {search_e}")
             except Exception as db_e:
                 print(f"âŒ Yerel Chroma veritabanÄ± kurulum hatasÄ±: {db_e}"); chroma_client = None; chroma_collection = None
        else:
            print("âŒ VeritabanÄ±na eklenecek geÃ§erli embedding bulunamadÄ±.")
    except Exception as e:
        print(f"âŒ Embedding/DB (AdÄ±m 5) genel hatasÄ±: {e}")
else:
    print("âŒ Ã–nceki adÄ±mlardaki hatalar (Chunk/API) nedeniyle AdÄ±m 5 atlandÄ±.")

if not DB_CREATED_OR_LOADED:
    print("\nâŒ AdÄ±m 5 tamamlanamadÄ±. VektÃ¶r VeritabanÄ± hazÄ±r deÄŸil.")
print("-" * 50)


# ==============================================================================
# AdÄ±m 6: RAG Pipeline (Zincir) Kurulumu
# ==============================================================================
# Bu adÄ±mda, tÃ¼m bileÅŸenleri (Retriever, Prompt, LLM) bir araya getirerek
# LangChain Expression Language (LCEL) kullanarak RAG zincirimizi kuruyoruz.
# Not: LLM olarak LangChain'in standart Gemini sarmalayÄ±cÄ±sÄ± (wrapper) yerine
# doÄŸrudan 'google.generativeai' kÃ¼tÃ¼phanesini kullanÄ±yoruz. Zincir buna gÃ¶re uyarlandÄ±.
print("\n--- AdÄ±m 6: RAG Pipeline Kurulumu ---")

# Bu importlar LCEL zinciri iÃ§in gereklidir (AdÄ±m 1'de yapÄ±ldÄ±)
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

rag_chain = None
RAG_READY = False
llm_model_name = "gemini-2.0-flash" # HÄ±z ve maliyet iÃ§in optimize edilmiÅŸ Gemini modeli

if DB_CREATED_OR_LOADED and chroma_collection is not None:
    print("RAG pipeline (zinciri) oluÅŸturuluyor...")
    try:
        # 1. LLM'i TanÄ±mla
        # DoÄŸrudan 'genai' kÃ¼tÃ¼phanesinden modeli Ã§aÄŸÄ±rÄ±yoruz.
        # 'temperature=0.3' ile modelin daha tutarlÄ± ve daha az yaratÄ±cÄ± (daha az "halÃ¼sinasyon")
        # cevaplar vermesini saÄŸlÄ±yoruz. Hukuk metinleri gibi hassas konular iÃ§in bu Ã¶nemlidir.
        llm_model = genai.GenerativeModel(
            model_name=llm_model_name,
            generation_config={"temperature": 0.3}
        )
        print(f"âœ… LLM ({llm_model_name}, T=0.3) tanÄ±mlandÄ±.")

        # 2. Retriever Fonksiyonunu TanÄ±mla
        # Bu fonksiyon, kullanÄ±cÄ± sorgusunu alÄ±r, ChromaDB'de arar
        # ve en ilgili 'k' adet metin parÃ§asÄ±nÄ± (context) dÃ¶ndÃ¼rÃ¼r.
        def retrieve_context(query: str, k: int = 3):
            # print(f"   [Debug: Retriever Ã§alÄ±ÅŸtÄ±, sorgu: '{query[:50]}...']") # (Debug iÃ§in)
            try:
                # Sorguyu, 'RETRIEVAL_QUERY' tipiyle embed et
                query_embedding = embed_content_with_retry(query, task_type='RETRIEVAL_QUERY')
                if not query_embedding:
                    return "Sorgu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lemedi."

                # ChromaDB'den en yakÄ±n 'k' sonucu (embedding'e gÃ¶re) sorgula
                # Sadece 'documents' (metinler) bÃ¶lÃ¼mÃ¼nÃ¼ iste
                results = chroma_collection.query(
                    query_embeddings=[query_embedding],
                    n_results=k,
                    include=['documents']
                )

                # SonuÃ§larÄ± formatla (birleÅŸtir)
                if results and results.get('documents') and results['documents'][0]:
                    context = "\n\n".join(results['documents'][0])
                    # print(f"   [Debug: {len(results['documents'][0])} adet baÄŸlam (context) bulundu.]") # (Debug iÃ§in)
                    return context
                else:
                    # print("   [Debug: Ä°lgili baÄŸlam bulunamadÄ±.]") # (Debug iÃ§in)
                    return "Ä°lgili bilgi bulunamadÄ±."
            except Exception as e:
                return f"Bilgi alÄ±nÄ±rken hata oluÅŸtu: {e}"
        print("âœ… Retriever (Bilgi Ã‡ekici) fonksiyonu tanÄ±mlandÄ±.")

        # 3. Prompt Åablonunu OluÅŸtur
        # Bu ÅŸablon, LLM'e tam olarak nasÄ±l davranmasÄ± gerektiÄŸini sÃ¶yler (System Prompt).
        # {context} ve {question} alanlarÄ± zincir tarafÄ±ndan doldurulacaktÄ±r.
        template_str = """
        YalnÄ±zca aÅŸaÄŸÄ±da verilen baÄŸlamÄ± (context) kullanarak soruyu TÃ¼rkÃ§e yanÄ±tlayÄ±n.
        EÄŸer cevap baÄŸlamda yoksa veya cevaptan emin deÄŸilseniz, "Bu konuda saÄŸlanan bilgiler arasÄ±nda bir cevap bulamadÄ±m." deyin.
        CevabÄ±nÄ±zÄ± doÄŸrudan ve net bir ÅŸekilde verin.

        BaÄŸlam:
        {context}

        Soru:
        {question}

        YanÄ±t:
        """
        prompt_template = PromptTemplate.from_template(template_str)
        print("âœ… Prompt ÅŸablonu (System Prompt) oluÅŸturuldu.")

        # 4. RAG Zincirini (LCEL) Kur
        # LCEL (LangChain Expression Language), '|' (pipe) operatÃ¶rÃ¼ ile
        # adÄ±mlarÄ± birbirine baÄŸlamamÄ±zÄ± saÄŸlar.
        # Zincir ÅŸu adÄ±mlarÄ± izler:
        rag_chain = (
            # AdÄ±m A: Zincir, 'question' iÃ§eren bir dictionary ile baÅŸlar.
            # RunnablePassthrough() gelen ham sorguyu (string) alÄ±r ve 'question' anahtarÄ± altÄ±na koyar.
            # Girdi: "Devletin ÅŸekli nedir?"
            # Ã‡Ä±ktÄ±: {"question": "Devletin ÅŸekli nedir?"}
            {"question": RunnablePassthrough()}

            # AdÄ±m B: 'context' anahtarÄ±nÄ± ekle.
            # 'retrieve_context' fonksiyonunu 'question' ile Ã§alÄ±ÅŸtÄ±r ve sonucunu 'context'e ata.
            # Girdi: {"question": "..."}
            # Ã‡Ä±ktÄ±: {"question": "...", "context": "[ilgili hukuk metni...]"}
            | RunnablePassthrough.assign(context=lambda x: retrieve_context(x["question"]))

            # AdÄ±m C: Prompt'u formatla.
            # {'question': ..., 'context': ...} girdisini alÄ±p prompt ÅŸablonuna yerleÅŸtirir.
            # Ã‡Ä±ktÄ± olarak formatlanmÄ±ÅŸ tam bir metin (string) verir.
            # Girdi: {"question": "...", "context": "..."}
            # Ã‡Ä±ktÄ±: "YalnÄ±zca aÅŸaÄŸÄ±da verilen baÄŸlamÄ±... Soru: ... YanÄ±t:"
            | RunnableLambda(lambda x: prompt_template.format(question=x["question"], context=x["context"]))

            # AdÄ±m D: LLM'i Ã‡aÄŸÄ±r.
            # FormatlanmÄ±ÅŸ metni (prompt) alÄ±r ve doÄŸrudan genai modeline gÃ¶nderir.
            # Modelden gelen '.text' yanÄ±tÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
            # Girdi: "YalnÄ±zca aÅŸaÄŸÄ±da verilen baÄŸlamÄ±..."
            # Ã‡Ä±ktÄ±: "TÃ¼rkiye Devleti bir Cumhuriyettir."
            | RunnableLambda(lambda formatted_prompt: llm_model.generate_content(formatted_prompt).text)
        )
        print("âœ… RAG zinciri (LCEL) baÅŸarÄ±yla oluÅŸturuldu.")

        # 5. Zinciri Test Et
        print("\nRAG zinciri test ediliyor (Ä°lk sorgu)...")
        test_question = "Devletin ÅŸekli nedir?"
        print(f"Test Sorusu: {test_question}")
        try:
            start_time = time.time()
            # 'invoke' metodu ile zinciri Ã§alÄ±ÅŸtÄ±r
            response = rag_chain.invoke(test_question)
            end_time = time.time()
            print(f"Test CevabÄ± ({end_time - start_time:.2f} s): {response}")
            RAG_READY = True
            if "bulunamadÄ±" in response:
                print("âš ï¸ Test cevabÄ± bilgi bulamadÄ± (Bu durum, veritabanÄ±nÄ±n ilgili bilgiyi iÃ§ermemesi veya retriever'Ä±n bulamamasÄ± durumunda normaldir).")
        except Exception as invoke_e:
            print(f"âŒ RAG zinciri testi (invoke) sÄ±rasÄ±nda hata: {invoke_e}")

    except Exception as e:
        print(f"âŒ RAG pipeline (AdÄ±m 6) kurulum hatasÄ±: {e}"); rag_chain = None
else:
    print("âŒ Ã–nceki adÄ±mlarda (VeritabanÄ±) hata olduÄŸu iÃ§in RAG zinciri kurulamadÄ±.")

if not RAG_READY:
    print("\nâŒ AdÄ±m 6 tamamlanamadÄ±. Chatbot hazÄ±r deÄŸil.")
print("-" * 50)


# ==============================================================================
# ==============================================================================
# *** Ä°KÄ°NCÄ° HÃœCRE BURADA BAÅLIYOR (AdÄ±m 7-9) ***
# ==============================================================================
# ==============================================================================


# ==============================================================================
# AdÄ±m 7: Streamlit ArayÃ¼z Kodu (app.py - GÃ¼venlik AyarlarÄ± Eklendi)
# ==============================================================================
# Bu adÄ±m, RAG modelimizi interaktif bir web arayÃ¼zÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼recek olan
# 'app.py' dosyasÄ±nÄ±n iÃ§eriÄŸini hazÄ±rlar.
# Kod, bir Python string'i (f-string) olarak oluÅŸturulur ve Colab ortamÄ±na
# bir dosya olarak yazÄ±lÄ±r.
print("\n--- AdÄ±m 7: Streamlit ArayÃ¼z Kodu HazÄ±rlanÄ±yor (GÃ¼venlik AyarlarÄ±) ---")

# Colab notebook'undaki mevcut deÄŸiÅŸkenleri (model adÄ±, DB yolu vb.)
# 'app.py' iÃ§ine gÃ¶mmek iÃ§in burada yakalÄ±yoruz.
APP_LLM_MODEL = "gemini-2.0-flash"
# 'DB_CREATED_OR_LOADED' bayraÄŸÄ±nÄ± kontrol et, eÄŸer DB yolu tanÄ±mlÄ±ysa onu kullan,
# deÄŸilse varsayÄ±lan (fallback) bir yol belirle.
APP_DB_PATH = db_path if 'DB_CREATED_OR_LOADED' in locals() and DB_CREATED_OR_LOADED else "./chroma_db_law_local_full"
APP_COLLECTION_NAME = collection_name if 'DB_CREATED_OR_LOADED' in locals() and DB_CREATED_OR_LOADED else "hukuk_tr_collection_full_local"
APP_SORU_COL = SORU_COLUMN if 'SORU_COLUMN' in locals() else 'Soru'
APP_DATASET_ID = dataset_name if 'dataset_name' in locals() else "Renicames/turkish-law-chatbot"

# Streamlit uygulamasÄ±nÄ±n tam Python kodunu iÃ§eren f-string:
# *** DÄ°KKAT: Bu f-string iÃ§indeki yorumlarda {} parantezleri kullanÄ±lmamalÄ±dÄ±r. ***
streamlit_app_code = f"""
# ===============================================
#     *** TÃœRK HUKUKU RAG CHATBOT - APP.PY ***
# ===============================================
# Bu dosya, Streamlit kÃ¼tÃ¼phanesini kullanarak
# RAG modelimiz iÃ§in bir web arayÃ¼zÃ¼ saÄŸlar.
# Not: Bu dosya, Colab notebook'u tarafÄ±ndan
# otomatik olarak oluÅŸturulmuÅŸtur.
# ===============================================

import streamlit as st
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold # Gemini GÃ¼venlik AyarlarÄ± iÃ§in
import chromadb
from langchain_core.prompts import PromptTemplate
import time
import os
import shutil # Dosya iÅŸlemleri iÃ§in

# --- KonfigÃ¼rasyon ve Global DeÄŸiÅŸkenler ---

# API AnahtarÄ±nÄ± al: Ã–nce ortam deÄŸiÅŸkenlerinden (os.environ) dener,
# bulamazsa Streamlit'in kendi 'secrets' (st.secrets) yÃ¶netiminden dener.
API_KEY = os.environ.get('GEMINI_API_KEY')
if not API_KEY:
    try:
        API_KEY = st.secrets["GEMINI_API_KEY"]
    except:
        st.error("Gemini API AnahtarÄ± 'GEMINI_API_KEY' bulunamadÄ±! LÃ¼tfen ayarlayÄ±n.")
        st.stop() # Anahtar yoksa uygulama durur.

# Colab'den enjekte edilen sabitler
DB_PATH = "{APP_DB_PATH}"
COLLECTION_NAME = "{APP_COLLECTION_NAME}"
MODEL_NAME_LLM = "{APP_LLM_MODEL}"
MODEL_NAME_EMBEDDING = "models/text-embedding-004"
SORU_COLUMN_NAME = "{APP_SORU_COL}"
DATASET_ID_STR = "{APP_DATASET_ID}"

# Gemini API'yi yapÄ±landÄ±r
try:
    genai.configure(api_key=API_KEY)
except Exception as config_e:
    st.error(f"API yapÄ±landÄ±rÄ±lamadÄ±: {{config_e}}")
    st.stop()

# --- YardÄ±mcÄ± Fonksiyonlar ---

# Embedding Fonksiyonu (Colab notebook'u ile aynÄ±)
# API hatalarÄ±na (Ã¶rn: rate limit) karÅŸÄ± yeniden deneme (retry) mekanizmasÄ± iÃ§erir.
def embed_content_with_retry(content, model=MODEL_NAME_EMBEDDING, task_type="RETRIEVAL_DOCUMENT", max_retries=3, initial_delay=1):
    delay = initial_delay; last_exception = None; is_batch = isinstance(content, list)
    for attempt in range(max_retries):
        try:
            current_task_type = task_type if isinstance(content, str) and task_type == 'RETRIEVAL_QUERY' else 'RETRIEVAL_DOCUMENT'
            result = genai.embed_content(model=model, content=content, task_type=current_task_type)
            embedding_key = 'embedding' if 'embedding' in result else ('embeddings' if 'embeddings' in result else None)
            if embedding_key: return result[embedding_key]
            else: raise ValueError("Embedding sonucu anahtar iÃ§ermiyor.")
        except Exception as e:
            # Hata durumunda kullanÄ±cÄ±ya 'toast' bildirimi gÃ¶ster
            st.toast(f"Embedding hatasÄ± (Deneme {{{{attempt + 1}}}}): {{{{e}}}}", icon="âš ï¸")
            last_exception = e
            if attempt < max_retries - 1:
                time.sleep(delay); delay *= 2 # Hata sonrasÄ± bekleme (exponential backoff)
            else:
                if is_batch: return [None] * len(content)
                else: raise last_exception
    return None

# Chroma VektÃ¶r VeritabanÄ±nÄ± YÃ¼kleme Fonksiyonu
# '@st.cache_resource': Streamlit'e bu fonksiyonun sonucunu (yani DB baÄŸlantÄ±sÄ±nÄ±)
# Ã¶nbelleÄŸe almasÄ±nÄ± (cache) sÃ¶yler. Bu sayede her kullanÄ±cÄ± etkileÅŸiminde
# veritabanÄ± tekrar tekrar yÃ¼klenmez, performans artar.
@st.cache_resource
def get_chroma_collection():
    try:
        # Streamlit deploy ortamlarÄ±nda dosya yollarÄ± deÄŸiÅŸebilir.
        # Ã–nce 'DB_PATH' var mÄ± diye kontrol et.
        if not os.path.exists(DB_PATH):
            # EÄŸer yoksa, 'app.py' dosyasÄ±nÄ±n bulunduÄŸu dizine gÃ¶re
            # gÃ¶receli (relative) yolu bulmaya Ã§alÄ±ÅŸ.
            script_dir = os.path.dirname(__file__)
            db_path_rel = os.path.join(script_dir, DB_PATH)
            if not os.path.exists(db_path_rel):
                 st.error(f"VeritabanÄ± yolu bulunamadÄ±: '{{DB_PATH}}' veya '{{db_path_rel}}'.")
                 return None
            effective_db_path = db_path_rel # GÃ¶receli yolu kullan
        else:
            effective_db_path = DB_PATH # Orijinal yolu kullan

        # 'PersistentClient', veritabanÄ±nÄ±n diskten (belirtilen yoldan)
        # kalÄ±cÄ± olarak yÃ¼klenmesini saÄŸlar. (Colab'de oluÅŸturduÄŸumuz DB)
        client = chromadb.PersistentClient(path=effective_db_path)
        # Koleksiyonu isimle Ã§aÄŸÄ±r
        collection = client.get_collection(name=COLLECTION_NAME)
        st.info(f"Yerel Chroma koleksiyonu '{{COLLECTION_NAME}}' yÃ¼klendi (Ã–ÄŸe SayÄ±sÄ±: {{collection.count()}}).")
        # DB'nin boÅŸ olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        if collection.count() == 0:
            st.error("âš ï¸ Yerel Chroma koleksiyonu boÅŸ! Colab'de AdÄ±m 5'in Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")
        return collection
    except Exception as e:
        st.error(f"Yerel Chroma yÃ¼klenirken/alÄ±nÄ±rken hata: {{e}}")
        st.error(f"VeritabanÄ± dosyalarÄ±nÄ±n ('{{DB_PATH}}' klasÃ¶rÃ¼) 'app.py' ile aynÄ± dizinde olduÄŸundan emin olun.")
        return None

# Retriever Fonksiyonu (Bilgi Ã‡ekici)
# Bu fonksiyon, kullanÄ±cÄ± sorgusuna en Ã§ok benzeyen metin parÃ§alarÄ±nÄ± (context) DB'den Ã§eker.
# GÃœNCELLEME: Bu versiyon, metinlere ek olarak kaynak (metadata) bilgisini de dÃ¶ndÃ¼rÃ¼r.
def retrieve_context(query: str, collection, k: int = 3):
    if collection is None:
        return "VeritabanÄ± baÄŸlantÄ±sÄ± kurulamadÄ±.", ""
    try:
        # 1. KullanÄ±cÄ± sorgusunu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼r (Query embedding)
        query_embedding = embed_content_with_retry(query, task_type='RETRIEVAL_QUERY')
        if not query_embedding:
            return "Sorgu vektÃ¶re Ã§evrilirken hata oluÅŸtu.", ""

        # 2. ChromaDB'de en yakÄ±n 'k' sonucu sorgula
        # 'include' ile hem metinleri (documents) hem de metaveriyi (metadatas) istiyoruz.
        results = collection.query(query_embeddings=[query_embedding], n_results=k, include=['documents', 'metadatas'])

        # 3. SonuÃ§larÄ± formatla
        if results and results.get('documents') and results['documents'][0]:
            context_list = []; sources = []
            # Bulunan her sonuÃ§ iÃ§in metni ve metaveriyi ayÄ±kla
            for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
                # Metaveriden orijinal 'Soru' sÃ¼tununu kaynak olarak al
                source_info = metadata.get(SORU_COLUMN_NAME, f"ID: {{metadata.get('source_id', 'Bilinmiyor')}}")
                context_list.append(f"Metin: {{doc}}")
                sources.append(source_info)

            # Bulunan metinleri LLM'in anlayacaÄŸÄ± 'context' formatÄ±na getir
            context_str = "\\n\\n".join(context_list)
            # KaynaklarÄ± (tekrarlarÄ± kaldÄ±rarak) formatla
            source_str = "\\n".join([f"- {{s}}" for s in set(sources)])

            return context_str, source_str # Hem baÄŸlamÄ± hem kaynaklarÄ± dÃ¶ndÃ¼r
        else:
            return "Ä°lgili bilgi bulunamadÄ±.", ""
    except Exception as e:
        st.error(f"Retriever hatasÄ±: {{e}}");
        return f"Bilgi alÄ±nÄ±rken hata oluÅŸtu.", ""

# Prompt Åablonu (RAG iÃ§in)
# Bu ÅŸablon, LLM'e nasÄ±l davranmasÄ± gerektiÄŸini sÃ¶yler (Sistem TalimatÄ±).
# Not: template_str_app iÃ§indeki {{{{ }}}} (4 kÃ¼me parantezi)
# Python'un f-string formatlamasÄ±ndan (Colab tarafÄ±) kaÃ§mak iÃ§indir.
template_str_app = '''Sen TÃ¼rk Hukuku alanÄ±nda uzman bir yapay zeka asistanÄ±sÄ±n...
BaÄŸlam:
{{{{context}}}}
Soru:
{{{{question}}}}
YanÄ±t:
'''
# *** HATANIN OLDUÄU YORUM SATIRI DÃœZELTÄ°LDÄ° ***
# Buradaki .replace() iÅŸlemi, ÅŸablonu LangChain'in anlayacaÄŸÄ±
# standart formata (yani 'context' ve 'question' anahtarlarÄ±na) getirir.
prompt_template_lc = PromptTemplate.from_template(template_str_app.replace('{{{{', '{{').replace('}}}}', '}}'))

# --- Streamlit ArayÃ¼zÃ¼ BaÅŸlangÄ±cÄ± ---
st.set_page_config(page_title="ğŸ‡¹ğŸ‡· TÃ¼rk Hukuku RAG Chatbot", page_icon="âš–")
st.title("âš– TÃ¼rk Hukuku RAG Chatbot")
st.caption(f"Veri Seti: {{DATASET_ID_STR}} (HF) | VektÃ¶r DB: Yerel Chroma | Model: {{MODEL_NAME_LLM}}")

# --- YENÄ°: Gemini GÃ¼venlik AyarlarÄ± (Safety Settings) ---
# Gemini modelleri varsayÄ±lan olarak katÄ± gÃ¼venlik filtrelerine sahiptir.
# Hukuk metinleri bazen (Ã¶rn: ceza hukuku) hassas/ÅŸiddet iÃ§eren terimler
# barÄ±ndÄ±rabileceÄŸinden, modelin cevap vermesini engellememesi iÃ§in
# filtreleri 'BLOCK_ONLY_HIGH' (Sadece YÃ¼ksek OlasÄ±lÄ±klÄ± ZararÄ± Engelle)
# seviyesine Ã§ekiyoruz.
safety_settings = {{
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
}}
# --------------------------------------------------------

# LLM (Dil Modeli) YÃ¼kleme Fonksiyonu
# '@st.cache_resource': TÄ±pkÄ± DB gibi, LLM modelini de Ã¶nbelleÄŸe alÄ±r.
# Bu, sayfa her yenilendiÄŸinde modelin API'den tekrar Ã§ekilmesini engeller.
@st.cache_resource
def initialize_llm():
    try:
        # GÃœNCELLENDÄ°: Modeli, belirlediÄŸimiz 'safety_settings' ile baÅŸlatÄ±yoruz.
        llm_model = genai.GenerativeModel(
            model_name=MODEL_NAME_LLM,
            generation_config={{"temperature": 0.3}}, # YaratÄ±cÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k tut (tutarlÄ± cevaplar iÃ§in)
            safety_settings=safety_settings
            )
        st.success(f"Gemini modeli ({{MODEL_NAME_LLM}}) baÅŸarÄ±yla yÃ¼klendi (GÃ¼venlik: BLOCK_ONLY_HIGH).")
        return llm_model
    except Exception as e:
        st.error(f"LLM yÃ¼klenirken hata oluÅŸtu: {{e}}"); return None

# Ana bileÅŸenleri (LLM ve DB) yÃ¼kle
llm_model = initialize_llm()
chroma_collection = get_chroma_collection()

# EÄŸer LLM veya DB yÃ¼klenemezse, uygulama hata verip durur.
if llm_model is None or chroma_collection is None:
    st.error("Ana kaynaklar (LLM veya VektÃ¶r DB) yÃ¼klenemedi. Uygulama durduruluyor.")
    st.stop()

# RAG Cevap Fonksiyonu (Ana MantÄ±k)
# KullanÄ±cÄ± sorgusunu alÄ±p RAG sÃ¼recini iÅŸleten fonksiyon.
def get_response_from_rag(user_query):
    try:
        # 1. Bilgiyi Ã‡ek (Retrieve)
        # DB'den ilgili metinleri (context) ve kaynaklarÄ± (sources_str) al
        retrieved_context, sources_str = retrieve_context(user_query, chroma_collection)

        # 2. Prompt'u HazÄ±rla (Augment)
        # KullanÄ±cÄ± sorusu ve Ã§ekilen bilgiyi ÅŸablona yerleÅŸtir
        formatted_prompt = prompt_template_lc.format(question=user_query, context=retrieved_context)

        # 3. Cevap Ãœret (Generate)
        # HazÄ±rlanan prompt'u LLM'e gÃ¶nder
        response = llm_model.generate_content(formatted_prompt) # safety_settings=safety_settings)

        try:
            # CevabÄ±n metnini al
            answer = response.text

            # GÃœNCELLEME: GÃ¼venlik Engeli KontrolÃ¼
            # EÄŸer 'answer' boÅŸsa VE 'prompt_feedback' bir 'block_reason' (Engelleme Nedeni)
            # iÃ§eriyorsa, bu, cevabÄ±n Gemini gÃ¼venlik filtresine takÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.
            if not answer and response.prompt_feedback.block_reason:
                 st.warning(f"âš ï¸ YanÄ±t gÃ¼venlik nedeniyle engellendi: {{response.prompt_feedback.block_reason}}")
                 return "ÃœzgÃ¼nÃ¼m, Ã¼rettiÄŸim yanÄ±t gÃ¼venlik politikalarÄ±mÄ±z nedeniyle engellendi."

            # 4. KaynaklarÄ± Cevaba Ekle
            # EÄŸer kaynak bulunduysa ve cevap "bulunamadÄ±" deÄŸilse, kaynaklarÄ± cevabÄ±n sonuna ekle.
            if sources_str and "bulunamadÄ±" not in answer:
                 answer += f"\\n\\n---\\n*Kaynaklar (Ä°lgili Orijinal Sorular):*\\n{{sources_str}}"
            return answer

        except Exception as resp_e:
            st.error(f"YanÄ±t (response) iÅŸlenirken hata: {{resp_e}}")
            return "YanÄ±t alÄ±nÄ±rken bir hata oluÅŸtu."

    except Exception as e:
        st.error(f"RAG sÃ¼reci hatasÄ±: {{e}}")
        return f"ÃœzgÃ¼nÃ¼m, cevap Ã¼retilirken genel bir hata oluÅŸtu."

# --- Chat ArayÃ¼zÃ¼ MantÄ±ÄŸÄ± ---

# Chat geÃ§miÅŸini Streamlit'in 'session_state' hafÄ±zasÄ±nda tut
if "messages" not in st.session_state:
    st.session_state.messages = [{{ "role": "assistant", "content": "Merhaba! TÃ¼rk hukuku hakkÄ±nda ne Ã¶ÄŸrenmek istersiniz?" }}] # BaÅŸlangÄ±Ã§ mesajÄ±

# GeÃ§miÅŸteki mesajlarÄ± ekrana yazdÄ±r
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni giriÅŸ (prompt) al
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    # 1. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± geÃ§miÅŸe ve ekrana ekle
    st.session_state.messages.append({{"role": "user", "content": prompt}})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AsistanÄ±n cevabÄ±nÄ± hazÄ±rla
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # Cevap gelene kadar boÅŸ bir alan ayÄ±r
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yor... (VeritabanÄ± taranÄ±yor ve cevap Ã¼retiliyor)"):
            # Ana RAG fonksiyonunu Ã§aÄŸÄ±r
            assistant_response = get_response_from_rag(prompt)

        # 3. AsistanÄ±n cevabÄ±nÄ± ekrana ve geÃ§miÅŸe ekle
        message_placeholder.markdown(assistant_response)
    st.session_state.messages.append({{"role": "assistant", "content": assistant_response}})
"""
# --- 'app.py' DosyasÄ±nÄ± Yazma ---

# YukarÄ±da f-string ile hazÄ±rladÄ±ÄŸÄ±mÄ±z kod iÃ§eriÄŸini ('streamlit_app_code')
# 'app.py' adÄ±nda gerÃ§ek bir dosyaya yazÄ±yoruz.
APP_PY_CREATED = False
try:
    # dataset_id'nin tanÄ±mlÄ± olduÄŸundan emin ol
    if 'dataset_id' not in locals(): dataset_id = "Renicames/turkish-law-chatbot"

    with open("app.py", "w", encoding="utf-8") as f: f.write(streamlit_app_code)
    print("âœ… app.py dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu/gÃ¼ncellendi (HatalÄ± yorum dÃ¼zeltildi).")
    APP_PY_CREATED = True
except Exception as e: print(f"âŒ app.py dosyasÄ± yazÄ±lÄ±rken hata oluÅŸtu: {e}")
print("-" * 50)


# ==============================================================================
# AdÄ±m 8: Streamlit ArayÃ¼zÃ¼nÃ¼ Ã‡alÄ±ÅŸtÄ±rma (ngrok ile)
# ==============================================================================
# Bu adÄ±m, 'app.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±r ve 'ngrok' kullanarak
# Colab Ã¼zerinde Ã§alÄ±ÅŸan bu uygulamaya herkesin eriÅŸebileceÄŸi
# geÃ§ici bir genel (public) URL oluÅŸturur.
print("\n--- AdÄ±m 8: Streamlit ArayÃ¼zÃ¼nÃ¼ Ã‡alÄ±ÅŸtÄ±rma ---")

# Gerekli kÃ¼tÃ¼phaneler (Bu adÄ±m iÃ§in)
import subprocess
import threading
from pyngrok import ngrok, conf
import time
# Colab secrets importu AdÄ±m 2'de yapÄ±ldÄ±, userdata tanÄ±mlÄ± olmalÄ±
# from google.colab import userdata
import os

NGROK_READY = False
# Ã–nceki adÄ±mlarÄ±n baÅŸarÄ± bayraklarÄ±nÄ± (flag) kontrol et
db_created_flag = 'DB_CREATED_OR_LOADED' in locals() and DB_CREATED_OR_LOADED
rag_ready_flag = 'RAG_READY' in locals() and RAG_READY
app_py_created_flag = 'APP_PY_CREATED' in locals() and APP_PY_CREATED

# SADECE tÃ¼m Ã¶nceki adÄ±mlar (DB, RAG, app.py) baÅŸarÄ±lÄ±ysa devam et
if db_created_flag and rag_ready_flag and app_py_created_flag:
    print("TÃ¼m adÄ±mlar baÅŸarÄ±lÄ±. Ngrok ve Streamlit baÅŸlatÄ±lÄ±yor...")
    NGROK_AUTH_TOKEN = None

    # Ngrok'u kullanmak iÃ§in bir 'Authtoken' gerekir.
    # Bu token, ngrok.com dashboard'undan Ã¼cretsiz alÄ±nabilir.
    # Token'Ä± koda yazmamak iÃ§in kullanÄ±cÄ±dan manuel girmesini istiyoruz.
    NGROK_AUTH_TOKEN = input("LÃ¼tfen Ngrok Authtoken'Ä±nÄ±zÄ± Girin (URL: https://dashboard.ngrok.com/get-started/your-authtoken ): ")

    if not NGROK_AUTH_TOKEN:
        print("âŒ Ngrok token girilmedi. Streamlit baÅŸlatÄ±lamÄ±yor.")
    else:
        try:
            # Ngrok'u alÄ±nan token ile ayarla
            ngrok.set_auth_token(NGROK_AUTH_TOKEN); print("âœ… ngrok authtoken ayarlandÄ±.")

            # Colab ortamÄ±nda kalabilecek eski iÅŸlemleri temizle
            try:
                subprocess.run(["killall", "-q", "streamlit"], check=False); print("Mevcut Streamlit iÅŸlemleri durduruldu.")
                ngrok.kill(); print("Mevcut Ngrok tÃ¼nelleri kapatÄ±ldÄ±."); time.sleep(3) # Sistemlerin kapanmasÄ± iÃ§in kÄ±sa bekleme
            except:
                pass # Hata verirse (Ã¶rn: iÅŸlem yoksa) es geÃ§

            # Streamlit'i AyrÄ± Bir Thread'de (Ä°ÅŸ ParÃ§acÄ±ÄŸÄ±) BaÅŸlatma
            # Streamlit sunucusu 'run' komutuyla baÅŸlatÄ±ldÄ±ÄŸÄ±nda ana Colab
            # hÃ¼cresini kilitler. Bunu engellemek iÃ§in 'threading' kullanÄ±yoruz.
            def run_streamlit():
                print("Streamlit sunucusu arka planda (thread) baÅŸlatÄ±lÄ±yor...")
                if os.path.exists("app.py"):
                    # 'subprocess.Popen' ile streamlit komutunu Ã§alÄ±ÅŸtÄ±r
                    # '--server.port=8501' : Streamlit'in Ã§alÄ±ÅŸacaÄŸÄ± port
                    # '--server.headless=true' : TarayÄ±cÄ±yÄ± otomatik aÃ§mamasÄ±nÄ± sÃ¶yler
                    # 'stdout/stderr=subprocess.DEVNULL' : Ã‡Ä±ktÄ±larÄ± gizler (Colab'i kirletmemesi iÃ§in)
                    subprocess.Popen(["streamlit", "run", "app.py", "--server.port=8501", "--server.headless=true"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                else:
                    print("âŒ app.py dosyasÄ± bulunamadÄ±, Streamlit baÅŸlatÄ±lamadÄ±.")

            thread = threading.Thread(target=run_streamlit);
            thread.start() # Thread'i baÅŸlat

            print("Streamlit thread'i baÅŸlatÄ±ldÄ±, ngrok tÃ¼neli iÃ§in 7sn bekleniyor...");
            time.sleep(7) # Streamlit sunucusunun ayaÄŸa kalkmasÄ± iÃ§in zaman tanÄ±yoruz

            # Ngrok TÃ¼nelini AÃ§
            # Ngrok'a, yerelde 8501 portunda Ã§alÄ±ÅŸan uygulamayÄ± dÄ±ÅŸarÄ±ya aÃ§masÄ±nÄ± sÃ¶ylÃ¼yoruz.
            public_url = ngrok.connect(8501)
            print("="*70 + f"\nâœ…âœ…âœ… Streamlit arayÃ¼zÃ¼ hazÄ±r! EriÅŸmek iÃ§in tÄ±klayÄ±n: {public_url}\n" + "="*70)
            NGROK_READY = True

        except Exception as e:
            print(f"âŒ ngrok veya Streamlit baÅŸlatÄ±lÄ±rken kritik hata: {e}")
else:
    # EÄŸer Ã¶nceki adÄ±mlardan biri baÅŸarÄ±sÄ±zsa, nedenini listele
    print("âŒ Ã–nceki adÄ±mlarda hata olduÄŸu iÃ§in Streamlit baÅŸlatÄ±lamÄ±yor.")
    if not db_created_flag:
        print("   - Neden: AdÄ±m 5 (VektÃ¶r VeritabanÄ±) baÅŸarÄ±yla tamamlanmadÄ±.")
    if not rag_ready_flag:
        print("   - Neden: AdÄ±m 6 (RAG Pipeline) baÅŸarÄ±yla tamamlanmadÄ±.")
    if not app_py_created_flag:
        print("   - Neden: AdÄ±m 7 (app.py oluÅŸturma) baÅŸarÄ±yla tamamlanmadÄ±.")
print("-" * 50)

# ==============================================================================
# AdÄ±m 9: requirements.txt DosyasÄ± OluÅŸturma
# ==============================================================================
# Bu adÄ±m, projenin Ã§alÄ±ÅŸmasÄ± iÃ§in gereken tÃ¼m Python paketlerini
# ve versiyonlarÄ±nÄ± iÃ§eren bir 'requirements.txt' dosyasÄ± oluÅŸturur.
# Bu dosya, projenin baÅŸka bir ortamda (Ã¶rn: sunucu)
# kolayca kurulabilmesi iÃ§in standart bir yÃ¶ntemdir.
if NGROK_READY:
     print("\n--- AdÄ±m 9: requirements.txt DosyasÄ± OluÅŸturuluyor ---")
     # Projede kullanÄ±lan ana kÃ¼tÃ¼phanelerin listesi ve spesifik versiyonlarÄ±
     requirements_content = """google-generativeai~=0.8.5
chromadb~=0.5.4
datasets~=2.20.0
langchain-text-splitters~=0.3.11
langchain-core~=0.3.11
langchain-community~=0.2.19
streamlit~=1.39.0
sentence_transformers~=3.0.1
langchain~=0.2.22
python-dotenv~=1.0.1
pyngrok~=7.1.6
pandas"""
     try:
         # Ä°Ã§eriÄŸi 'requirements.txt' dosyasÄ±na yaz
         with open("requirements.txt", "w") as f:
             f.write(requirements_content.strip())
         print("âœ… requirements.txt dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu.")
         # 'cat' komutu ile dosyanÄ±n iÃ§eriÄŸini Colab'de gÃ¶ster (kontrol amaÃ§lÄ±)
         get_ipython().system('cat requirements.txt')
     except Exception as e:
         print(f"âŒ requirements.txt dosyasÄ± yazÄ±lÄ±rken hata: {e}")
     print("-" * 50)
     print("ğŸ TÃ¼m AdÄ±mlar TamamlandÄ±.")
     print("â¡ï¸ Chatbot'u kullanmak iÃ§in yukarÄ±daki Streamlit (ngrok) linkini kullanabilirsiniz.")
else:
    print("ğŸ AdÄ±mlar tamamlandÄ± ancak Streamlit arayÃ¼zÃ¼ baÅŸlatÄ±lamadÄ± (Detaylar AdÄ±m 8'de).")

