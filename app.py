# ===============================================
#     *** TÃœRK HUKUKU RAG CHATBOT - APP.PY ***
# ===============================================
# Bu dosya, Streamlit kÃ¼tÃ¼phanesini kullanarak
# RAG modelimiz iÃ§in bir web arayÃ¼zÃ¼ saÄŸlar.
# ===============================================

### DEBUG ###
print("--- APP.PY BAÅLADI ---")

import streamlit as st
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold # Gemini GÃ¼venlik AyarlarÄ± iÃ§in
import chromadb
from langchain_core.prompts import PromptTemplate
import time
import os
import shutil # Dosya iÅŸlemleri iÃ§in

### DEBUG ###
print("--- Importlar tamamlandÄ± ---")

# --- KonfigÃ¼rasyon ve Global DeÄŸiÅŸkenler ---

### DEBUG ###
print("--- KonfigÃ¼rasyon baÅŸlÄ±yor ---")

# API AnahtarÄ±nÄ± al: Ã–nce ortam deÄŸiÅŸkenlerinden (os.environ) dener,
# bulamazsa Streamlit'in kendi 'secrets' (st.secrets) yÃ¶netiminden dener.
API_KEY = os.environ.get('GEMINI_API_KEY')
if not API_KEY:
    try:
        API_KEY = st.secrets["GEMINI_API_KEY"]
        ### DEBUG ###
        print("--- API Key secrets'ten alÄ±ndÄ± ---")
    except Exception:
        ### DEBUG ###
        print("--- HATA: API Key bulunamadÄ± ---")
        st.error("Gemini API AnahtarÄ± 'GEMINI_API_KEY' bulunamadÄ±! LÃ¼tfen Hugging Face Space Secrets'e ekleyin.")
        st.stop() # Anahtar yoksa uygulama durur.
else:
    ### DEBUG ###
    print("--- API Key ortam deÄŸiÅŸkeninden alÄ±ndÄ± ---")


# Proje Sabitleri
DB_PATH = "./chroma_db_law_local_full"
COLLECTION_NAME = "hukuk_tr_collection_full_local"
MODEL_NAME_LLM = "gemini-2.0-flash"
MODEL_NAME_EMBEDDING = "models/text-embedding-004"
SORU_COLUMN_NAME = "Soru"
DATASET_ID_STR = "Renicames/turkish-law-chatbot"

# Gemini API'yi yapÄ±landÄ±r
try:
    genai.configure(api_key=API_KEY)
    ### DEBUG ###
    print("--- Gemini API yapÄ±landÄ±rÄ±ldÄ± ---")
except Exception as config_e:
    ### DEBUG ###
    print(f"--- HATA: Gemini API yapÄ±landÄ±rÄ±lamadÄ±: {config_e} ---")
    st.error(f"API yapÄ±landÄ±rÄ±lamadÄ±: {config_e}")
    st.stop()

# --- YardÄ±mcÄ± Fonksiyonlar ---

### DEBUG ###
print("--- YardÄ±mcÄ± fonksiyonlar tanÄ±mlanÄ±yor ---")

# Embedding Fonksiyonu
def embed_content_with_retry(content, model=MODEL_NAME_EMBEDDING, task_type="RETRIEVAL_DOCUMENT", max_retries=3, initial_delay=1):
    ### DEBUG ###
    # print(f"--- embed_content_with_retry Ã§aÄŸrÄ±ldÄ± (tip: {task_type}) ---") # Ã‡ok fazla log Ã¼retebilir, ÅŸimdilik kapalÄ±
    delay = initial_delay; last_exception = None; is_batch = isinstance(content, list)
    for attempt in range(max_retries):
        try:
            current_task_type = task_type if isinstance(content, str) and task_type == 'RETRIEVAL_QUERY' else 'RETRIEVAL_DOCUMENT'
            result = genai.embed_content(model=model, content=content, task_type=current_task_type)
            embedding_key = 'embedding' if 'embedding' in result else ('embeddings' if 'embeddings' in result else None)
            if embedding_key: return result[embedding_key]
            else: raise ValueError("Embedding sonucu anahtar iÃ§ermiyor.")
        except Exception as e:
            ### DEBUG ###
            print(f"--- HATA: Embedding deneme {attempt + 1}: {e} ---")
            st.toast(f"Embedding hatasÄ± (Deneme {attempt + 1}): {e}", icon="âš ï¸")
            last_exception = e
            if attempt < max_retries - 1:
                time.sleep(delay); delay *= 2 # Hata sonrasÄ± bekleme (exponential backoff)
            else:
                if is_batch: return [None] * len(content)
                else: raise last_exception
    return None

# Chroma VektÃ¶r VeritabanÄ±nÄ± YÃ¼kleme Fonksiyonu
@st.cache_resource
def get_chroma_collection():
    ### DEBUG ###
    print("--- get_chroma_collection fonksiyonu baÅŸladÄ± ---")
    try:
        ### DEBUG ###
        print(f"--- DB Yolu kontrol ediliyor: {DB_PATH} ---")
        # DB_PATH klasÃ¶rÃ¼nÃ¼n var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        if not os.path.exists(DB_PATH):
            ### DEBUG ###
            print(f"--- HATA: DB yolu bulunamadÄ±: {DB_PATH} ---")
            st.error(f"VeritabanÄ± yolu bulunamadÄ±: '{DB_PATH}'.")
            st.error("LÃ¼tfen Colab'de oluÅŸturduÄŸunuz 'chroma_db_law_local_full' klasÃ¶rÃ¼nÃ¼ bu reponun ana dizinine yÃ¼klediÄŸinizden emin olun.")
            return None
        
        effective_db_path = DB_PATH 
        ### DEBUG ###
        print(f"--- KullanÄ±lacak DB yolu: {effective_db_path} ---")

        # 'PersistentClient', veritabanÄ±nÄ±n diskten (belirtilen yoldan)
        # kalÄ±cÄ± olarak yÃ¼klenmesini saÄŸlar.
        ### DEBUG ###
        print("--- chromadb.PersistentClient oluÅŸturuluyor... ---")
        client = chromadb.PersistentClient(path=effective_db_path)
        ### DEBUG ###
        print("--- PersistentClient oluÅŸturuldu. ---")
        
        # Koleksiyonu isimle Ã§aÄŸÄ±r (oluÅŸturmaya Ã§alÄ±ÅŸma, SADECE yÃ¼kle)
        ### DEBUG ###
        print(f"--- Koleksiyon '{COLLECTION_NAME}' alÄ±nÄ±yor... ---")
        collection = client.get_collection(name=COLLECTION_NAME)
        ### DEBUG ###
        print(f"--- Koleksiyon alÄ±ndÄ±. Ã–ÄŸe sayÄ±sÄ±: {collection.count()} ---")
        
        st.info(f"Yerel Chroma koleksiyonu '{COLLECTION_NAME}' yÃ¼klendi (Ã–ÄŸe SayÄ±sÄ±: {collection.count()}).")
        
        # DB'nin boÅŸ olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        if collection.count() == 0:
            ### DEBUG ###
            print("--- UYARI: Chroma koleksiyonu boÅŸ! ---")
            st.error("âš ï¸ Yerel Chroma koleksiyonu boÅŸ! VeritabanÄ± dosyalarÄ± bozuk veya yanlÄ±ÅŸ yÃ¼klenmiÅŸ olabilir.")
        
        ### DEBUG ###
        print("--- get_chroma_collection fonksiyonu baÅŸarÄ±yla tamamlandÄ± ---")
        return collection
        
    except Exception as e:
        ### DEBUG ###
        print(f"--- HATA: Chroma yÃ¼klenirken: {e} ---")
        st.error(f"Yerel Chroma yÃ¼klenirken/alÄ±nÄ±rken hata: {e}")
        st.error(f"VeritabanÄ± dosyalarÄ±nÄ±n ('{DB_PATH}' klasÃ¶rÃ¼) 'app.py' ile aynÄ± dizinde olduÄŸundan emin olun.")
        return None

# Retriever Fonksiyonu (Bilgi Ã‡ekici)
def retrieve_context(query: str, collection, k: int = 3):
    ### DEBUG ###
    # print(f"--- retrieve_context Ã§aÄŸrÄ±ldÄ±: '{query[:50]}...' ---") # Ã‡ok fazla log Ã¼retebilir
    if collection is None:
        ### DEBUG ###
        print("--- retrieve_context: Koleksiyon None, Ã§Ä±kÄ±lÄ±yor. ---")
        return "VeritabanÄ± baÄŸlantÄ±sÄ± kurulamadÄ±.", ""
    try:
        query_embedding = embed_content_with_retry(query, task_type='RETRIEVAL_QUERY')
        if not query_embedding:
            ### DEBUG ###
            print("--- retrieve_context: Sorgu embed edilemedi. ---")
            return "Sorgu vektÃ¶re Ã§evrilirken hata oluÅŸtu.", ""

        results = collection.query(query_embeddings=[query_embedding], n_results=k, include=['documents', 'metadatas'])

        if results and results.get('documents') and results['documents'][0]:
            context_list = []; sources = []
            for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
                source_info = metadata.get(SORU_COLUMN_NAME, f"ID: {metadata.get('source_id', 'Bilinmiyor')}")
                context_list.append(f"Metin: {doc}")
                sources.append(source_info)

            context_str = "\n\n".join(context_list)
            source_str = "\n".join([f"- {s}" for s in set(sources)])
            ### DEBUG ###
            # print(f"--- retrieve_context: {len(sources)} kaynak bulundu. ---")
            return context_str, source_str
        else:
            ### DEBUG ###
            print("--- retrieve_context: Ä°lgili bilgi bulunamadÄ±. ---")
            return "Ä°lgili bilgi bulunamadÄ±.", ""
    except Exception as e:
        ### DEBUG ###
        print(f"--- HATA: Retriever: {e} ---")
        st.error(f"Retriever hatasÄ±: {e}");
        return f"Bilgi alÄ±nÄ±rken hata oluÅŸtu.", ""

# Prompt Åablonu (RAG iÃ§in)
template_str_app = '''Sen TÃ¼rk Hukuku alanÄ±nda uzman bir yapay zeka asistanÄ±sÄ±n...
BaÄŸlam:
{context}
Soru:
{question}
YanÄ±t:
'''
prompt_template_lc = PromptTemplate.from_template(template_str_app)

### DEBUG ###
print("--- Prompt ÅŸablonu oluÅŸturuldu ---")

# --- Streamlit ArayÃ¼zÃ¼ BaÅŸlangÄ±cÄ± ---
st.set_page_config(page_title="ğŸ‡¹ğŸ‡· TÃ¼rk Hukuku RAG Chatbot", page_icon="âš–")
st.title("âš– TÃ¼rk Hukuku RAG Chatbot")
st.caption(f"Veri Seti: {DATASET_ID_STR} (HF) | VektÃ¶r DB: Yerel Chroma | Model: {MODEL_NAME_LLM}")

### DEBUG ###
print("--- Streamlit sayfa konfigÃ¼rasyonu yapÄ±ldÄ± ---")

# --- Gemini GÃ¼venlik AyarlarÄ± (Safety Settings) ---
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
}
# --------------------------------------------------------

### DEBUG ###
print("--- GÃ¼venlik ayarlarÄ± tanÄ±mlandÄ± ---")

# LLM (Dil Modeli) YÃ¼kleme Fonksiyonu
@st.cache_resource
def initialize_llm():
    ### DEBUG ###
    print("--- initialize_llm fonksiyonu baÅŸladÄ± ---")
    try:
        ### DEBUG ###
        print(f"--- GenerativeModel ({MODEL_NAME_LLM}) oluÅŸturuluyor... ---")
        llm_model = genai.GenerativeModel(
            model_name=MODEL_NAME_LLM,
            generation_config={"temperature": 0.3}, # YaratÄ±cÄ±lÄ±ÄŸÄ± dÃ¼ÅŸÃ¼k tut
            safety_settings=safety_settings
            )
        ### DEBUG ###
        print("--- GenerativeModel oluÅŸturuldu. ---")
        st.success(f"Gemini modeli ({MODEL_NAME_LLM}) baÅŸarÄ±yla yÃ¼klendi (GÃ¼venlik: BLOCK_ONLY_HIGH).")
        ### DEBUG ###
        print("--- initialize_llm fonksiyonu baÅŸarÄ±yla tamamlandÄ± ---")
        return llm_model
    except Exception as e:
        ### DEBUG ###
        print(f"--- HATA: LLM yÃ¼klenirken: {e} ---")
        st.error(f"LLM yÃ¼klenirken hata oluÅŸtu: {e}"); return None

# Ana bileÅŸenleri (LLM ve DB) yÃ¼kle
### DEBUG ###
print("--- Ana bileÅŸenler yÃ¼kleniyor (LLM)... ---")
llm_model = initialize_llm()
### DEBUG ###
print(f"--- LLM yÃ¼klemesi tamamlandÄ±. SonuÃ§: {'BaÅŸarÄ±lÄ±' if llm_model else 'BaÅŸarÄ±sÄ±z'} ---")

### DEBUG ###
print("--- Ana bileÅŸenler yÃ¼kleniyor (Chroma DB)... ---")
chroma_collection = get_chroma_collection()
### DEBUG ###
print(f"--- Chroma DB yÃ¼klemesi tamamlandÄ±. SonuÃ§: {'BaÅŸarÄ±lÄ±' if chroma_collection else 'BaÅŸarÄ±sÄ±z'} ---")


if llm_model is None or chroma_collection is None:
    ### DEBUG ###
    print("--- HATA: LLM veya Chroma yÃ¼klenemedi, uygulama durduruluyor. ---")
    st.error("Ana kaynaklar (LLM veya VektÃ¶r DB) yÃ¼klenemedi. Uygulama durduruluyor.")
    st.stop()

### DEBUG ###
print("--- Ana bileÅŸenler baÅŸarÄ±yla yÃ¼klendi ---")

# RAG Cevap Fonksiyonu (Ana MantÄ±k)
def get_response_from_rag(user_query):
    ### DEBUG ###
    print(f"--- get_response_from_rag Ã§aÄŸrÄ±ldÄ±: '{user_query[:50]}...' ---")
    try:
        # 1. Bilgiyi Ã‡ek (Retrieve)
        ### DEBUG ###
        print("--- Retriever Ã§aÄŸrÄ±lÄ±yor... ---")
        retrieved_context, sources_str = retrieve_context(user_query, chroma_collection)
        ### DEBUG ###
        print(f"--- Retriever dÃ¶ndÃ¼. Context var mÄ±?: {bool(retrieved_context and 'bulunamadÄ±' not in retrieved_context)}, Kaynak var mÄ±?: {bool(sources_str)} ---")

        # 2. Prompt'u HazÄ±rla (Augment)
        formatted_prompt = prompt_template_lc.format(question=user_query, context=retrieved_context)
        ### DEBUG ###
        # print(f"--- FormatlanmÄ±ÅŸ Prompt (ilk 100 char): {formatted_prompt[:100]}... ---") # Ã‡ok uzun olabilir

        # 3. Cevap Ãœret (Generate)
        ### DEBUG ###
        print("--- LLM generate_content Ã§aÄŸrÄ±lÄ±yor... ---")
        response = llm_model.generate_content(formatted_prompt)
        ### DEBUG ###
        print("--- LLM generate_content dÃ¶ndÃ¼. ---")

        try:
            answer = response.text
            ### DEBUG ###
            print(f"--- LLM CevabÄ± (ilk 50 char): {answer[:50]}... ---")
            
            # GÃ¼venlik Engeli KontrolÃ¼
            if not answer and response.prompt_feedback.block_reason:
                ### DEBUG ###
                print(f"--- UYARI: YanÄ±t gÃ¼venlik nedeniyle engellendi: {response.prompt_feedback.block_reason} ---")
                st.warning(f"âš ï¸ YanÄ±t gÃ¼venlik nedeniyle engellendi: {response.prompt_feedback.block_reason}")
                return "ÃœzgÃ¼nÃ¼m, Ã¼rettiÄŸim yanÄ±t gÃ¼venlik politikalarÄ±mÄ±z nedeniyle engellendi."
            
            # 4. KaynaklarÄ± Cevaba Ekle
            if sources_str and "bulunamadÄ±" not in answer:
                answer += f"\n\n---\n*Kaynaklar (Ä°lgili Orijinal Sorular):*\n{sources_str}"
                ### DEBUG ###
                print("--- Kaynaklar cevaba eklendi. ---")
            return answer

        except Exception as resp_e:
            ### DEBUG ###
            print(f"--- HATA: YanÄ±t iÅŸlenirken: {resp_e} ---")
            st.error(f"YanÄ±t (response) iÅŸlenirken hata: {resp_e}")
            st.warning(f"Modelin ham cevabÄ±: {response}")
            return "YanÄ±t alÄ±nÄ±rken bir hata oluÅŸtu."

    except Exception as e:
        ### DEBUG ###
        print(f"--- HATA: RAG sÃ¼reci genel hata: {e} ---")
        st.error(f"RAG sÃ¼reci hatasÄ±: {e}")
        return f"ÃœzgÃ¼nÃ¼m, cevap Ã¼retilirken genel bir hata oluÅŸtu."

# --- Chat ArayÃ¼zÃ¼ MantÄ±ÄŸÄ± ---

### DEBUG ###
print("--- Chat arayÃ¼zÃ¼ mantÄ±ÄŸÄ± baÅŸlÄ±yor ---")

# Chat geÃ§miÅŸini Streamlit'in 'session_state' hafÄ±zasÄ±nda tut
if "messages" not in st.session_state:
    st.session_state.messages = [{ "role": "assistant", "content": "Merhaba! TÃ¼rk hukuku hakkÄ±nda ne Ã¶ÄŸrenmek istersiniz?" }] # BaÅŸlangÄ±Ã§ mesajÄ±

# GeÃ§miÅŸteki mesajlarÄ± ekrana yazdÄ±r
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni giriÅŸ (prompt) al
if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
    # 1. KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± geÃ§miÅŸe ve ekrana ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. AsistanÄ±n cevabÄ±nÄ± hazÄ±rla
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # Cevap gelene kadar boÅŸ bir alan ayÄ±r
        with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yor... (VeritabanÄ± taranÄ±yor ve cevap Ã¼retiliyor)"):
            ### DEBUG ###
            print("--- Spinner aktif, RAG fonksiyonu Ã§aÄŸrÄ±lÄ±yor ---")
            assistant_response = get_response_from_rag(prompt)
            ### DEBUG ###
            print("--- RAG fonksiyonu dÃ¶ndÃ¼, spinner kapanÄ±yor ---")
        
        # 3. AsistanÄ±n cevabÄ±nÄ± ekrana ve geÃ§miÅŸe ekle
        message_placeholder.markdown(assistant_response)
    st.session_state.messages.append({"role": "assistant", "content": assistant_response})

### DEBUG ###
print("--- APP.PY SONLANDI (Streamlit devralÄ±yor) ---")
